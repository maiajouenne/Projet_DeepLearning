{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Deep Learning\n",
    "by Thomas ODIN, Maïa JOUENNE et Benoit CATEZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import et telechargement des paquets necessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 10:37:00.018623: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-01 10:37:00.248255: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-01 10:37:00.248330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-01 10:37:00.281223: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-01 10:37:00.359026: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-01 10:37:01.013694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# For drawing onto the image.\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageColor\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageOps\n",
    "\n",
    "# For measuring the inference time.\n",
    "import time\n",
    "\n",
    "# augmenting dataset\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df_train = pd.read_csv('data/train/_annotations.csv')\n",
    "\n",
    "# Read and store cropped images\n",
    "cropped_image_list = []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    image_path = row['filename']\n",
    "    image = Image.open('data/train/' + image_path)\n",
    "\n",
    "    # Extract cropping coordinates\n",
    "    xmin, ymin, xmax, ymax = (\n",
    "        max(0, row['xmin']),\n",
    "        max(0, row['ymin']),\n",
    "        min(image.width, row['xmax']),\n",
    "        min(image.height, row['ymax'])\n",
    "    )\n",
    "\n",
    "    # Check if the adjusted coordinates are valid\n",
    "    if xmin < xmax and ymin < ymax:\n",
    "        # Crop the image\n",
    "        cropped_image = np.array(image.crop((xmin, ymin, xmax, ymax)))\n",
    "\n",
    "        # Store the cropped image in the list\n",
    "        cropped_image_list.append(cropped_image)\n",
    "    else:\n",
    "        # If the coordinates are invalid, append a placeholder (e.g., None)\n",
    "        cropped_image_list.append(None)\n",
    "\n",
    "# Add a new column to the DataFrame with cropped images\n",
    "df_train['cropped_image'] = cropped_image_list\n",
    "\n",
    "# Filter out rows with None values in the 'cropped_image' column\n",
    "df_valid_crops = df_train.dropna(subset=['cropped_image'])\n",
    "\n",
    "# Display general information\n",
    "print(\"Number of valid cropped images:\", len(df_valid_crops))\n",
    "print(\"Number of proposed images:\", len(df_train))\n",
    "print(\"Image sizes:\")\n",
    "print(df_valid_crops['cropped_image'].apply(lambda x: x.shape).value_counts())\n",
    "\n",
    "# Get unique classes in the DataFrame\n",
    "unique_classes = df_valid_crops['class'].unique()\n",
    "\n",
    "# Number of random images to plot for each class\n",
    "images_to_plot = 2\n",
    "\n",
    "# Plot two random images for each class\n",
    "for class_name in unique_classes:\n",
    "    # Filter DataFrame based on the current class\n",
    "    df_same_class = df_valid_crops[df_valid_crops['class'] == class_name]\n",
    "\n",
    "    # Check the number of images in the current class\n",
    "    num_images = len(df_same_class)\n",
    "\n",
    "    if num_images >= images_to_plot:\n",
    "        # Randomly select two images from the current class\n",
    "        random_indices = random.sample(df_same_class.index.tolist(), images_to_plot)\n",
    "\n",
    "        # Plot the two random images for the current class\n",
    "        fig, axes = plt.subplots(1, images_to_plot, figsize=(10, 5))\n",
    "        fig.suptitle(f'Two Random Images of Class: {class_name}')\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(df_same_class.loc[random_indices[i]]['cropped_image'])\n",
    "            ax.set_title(f\"Index: {random_indices[i]}\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    elif num_images == 1:\n",
    "        # Plot the single image for the current class\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(df_same_class.iloc[0]['cropped_image'])\n",
    "        plt.title(f'Image of Class: {class_name} (Index: {df_same_class.index[0]}), only one sample')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images of class '{class_name}' for plotting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the classes are the same but with different name and that some of the picture are very pixelized. So we are going to do some mapping for the classes and create a function to automate the recuperation of the data. We can also see that due to the cropping the image are all of different size so after we are going to resize them into the same size the biggest one. We can also see that some image are double but we won't do anythings about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_class_to_actual_class = {\n",
    "    'tuna' : 'tuna', \n",
    "    'surgeon': 'surgeon', \n",
    "    'shark': 'shark', \n",
    "    'jack': 'jack', \n",
    "    'grouper': 'grouper', \n",
    "    'parrot': 'parrot', \n",
    "    'snapper': 'snapper',\n",
    "    'damsel': 'damsel', \n",
    "    'trigger': 'trigger', \n",
    "    'Zanclidae (Moorish Idol)': 'moorish idol',\n",
    "    'Scaridae -Parrotfishes-': 'parrot', \n",
    "    'Carangidae -Jacks-': 'jack',\n",
    "    'Scombridae -Tunas-': 'tuna', \n",
    "    'Shark -Selachimorpha-': 'shark',\n",
    "    'Serranidae -Groupers-': 'grouper', \n",
    "    'Lutjanidae -Snappers-': 'snapper',\n",
    "    'Acanthuridae -Surgeonfishes-': 'surgeon', \n",
    "    'Pomacentridae -Damselfishes-': 'damsel',\n",
    "    'Labridae -Wrasse-': 'wrasse', \n",
    "    'angel': 'angel', \n",
    "    'wrasse': 'wrasse', \n",
    "    'Zanclidae -Moorish Idol-': 'moorish idol',\n",
    "    'Ephippidae -Spadefishes-': 'spade', \n",
    "    'Pomacanthidae -Angelfishes-': 'angel',\n",
    "    'Balistidae -Triggerfishes-': 'trigger', \n",
    "    'spade': 'spade'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy valid dataframe \n",
    "df_new_class = df_valid_crops.copy()\n",
    "\n",
    "# Replace old class names with new class names\n",
    "df_new_class['class'] = df_new_class['class'].replace(old_class_to_actual_class)\n",
    "\n",
    "# Get unique classes in the DataFrame\n",
    "unique_classes = df_new_class['class'].unique()\n",
    "\n",
    "# Number of random images to plot for each class\n",
    "images_to_plot = 2\n",
    "\n",
    "# Plot two random images for each class\n",
    "for class_name in unique_classes:\n",
    "    # Filter DataFrame based on the current class\n",
    "    df_same_class = df_new_class[df_new_class['class'] == class_name]\n",
    "\n",
    "    # Check the number of images in the current class\n",
    "    num_images = len(df_same_class)\n",
    "\n",
    "    if num_images >= images_to_plot:\n",
    "        # Randomly select two images from the current class\n",
    "        random_indices = random.sample(df_same_class.index.tolist(), images_to_plot)\n",
    "\n",
    "        # Plot the two random images for the current class\n",
    "        fig, axes = plt.subplots(1, images_to_plot, figsize=(10, 5))\n",
    "        fig.suptitle(f'Two Random Images of Class: {class_name}')\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(df_same_class.loc[random_indices[i]]['cropped_image'])\n",
    "            ax.set_title(f\"Index: {random_indices[i]}\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "    elif num_images == 1:\n",
    "        # Plot the single image for the current class\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(df_same_class.iloc[0]['cropped_image'])\n",
    "        plt.title(f'Image of Class: {class_name} (Index: {df_same_class.index[0]}), only one sample')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images of class '{class_name}' for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_class['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some of the data which is underreepresented as told in the website "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to look at the size of the images and reshape them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_crops['cropped_image'].apply(lambda x: x.shape).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the average image size\n",
    "index = df_valid_crops['cropped_image'].apply(lambda x: x.shape).value_counts().sort_index().index\n",
    "\n",
    "# Convert tuples to arrays\n",
    "index_as_arrays = np.array([np.array(x) for x in index])\n",
    "\n",
    "# Transpose the array to have dimensions in the order (height, width, channels)\n",
    "index_transposed = index_as_arrays.transpose()\n",
    "\n",
    "# Calculate the average along each dimension\n",
    "average_size = np.mean(index_transposed, axis=1)\n",
    "\n",
    "print(average_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target size for resizing\n",
    "target_size = (138, 171)  \n",
    "\n",
    "# Reshape the images in the 'cropped_image' column\n",
    "df_new_class['cropped_image'] = df_new_class['cropped_image'].apply(lambda x: cv2.resize(x, target_size) if x is not None else None)\n",
    "\n",
    "# Get unique classes in the DataFrame\n",
    "unique_classes = df_new_class['class'].unique()\n",
    "\n",
    "# Number of random images to plot for each class\n",
    "images_to_plot = 2\n",
    "\n",
    "# Plot two random images for each class\n",
    "for class_name in unique_classes:\n",
    "    # Filter DataFrame based on the current class\n",
    "    df_same_class = df_new_class[df_new_class['class'] == class_name]\n",
    "\n",
    "    # Check the number of images in the current class\n",
    "    num_images = len(df_same_class)\n",
    "\n",
    "    if num_images >= images_to_plot:\n",
    "        # Randomly select two images from the current class\n",
    "        random_indices = random.sample(df_same_class.index.tolist(), images_to_plot)\n",
    "\n",
    "        # Plot the two random images for the current class\n",
    "        fig, axes = plt.subplots(1, images_to_plot, figsize=(10, 5))\n",
    "        fig.suptitle(f'Two Random Images of Class: {class_name}')\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(df_same_class.loc[random_indices[i]]['cropped_image'])\n",
    "            ax.set_title(f\"Index: {random_indices[i]}\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "    elif num_images == 1:\n",
    "        # Plot the single image for the current class\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(df_same_class.iloc[0]['cropped_image'])\n",
    "        plt.title(f'Image of Class: {class_name} (Index: {df_same_class.index[0]}), only one sample')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images of class '{class_name}' for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for crop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_crop(path):\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    if path[-1] == '/':\n",
    "        df = pd.read_csv(path +'_annotations.csv')\n",
    "    else :\n",
    "        df = pd.read_csv(path +'/_annotations.csv')\n",
    "\n",
    "    # Read and store cropped images\n",
    "    cropped_image_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        image_path = row['filename']\n",
    "        if path[-1] == '/':\n",
    "            image = Image.open(path + image_path)\n",
    "        else :\n",
    "            image = Image.open(path + '/' + image_path)\n",
    "            \n",
    "\n",
    "        # Extract cropping coordinates\n",
    "        xmin, ymin, xmax, ymax = (\n",
    "            max(0, row['xmin']),\n",
    "            max(0, row['ymin']),\n",
    "            min(image.width, row['xmax']),\n",
    "            min(image.height, row['ymax'])\n",
    "        )\n",
    "\n",
    "        # Check if the adjusted coordinates are valid\n",
    "        if xmin < xmax and ymin < ymax:\n",
    "            \n",
    "            # Crop the image\n",
    "            cropped_image = np.array(image.crop((xmin, ymin, xmax, ymax))) \n",
    "\n",
    "            # Reshape the images in the 'cropped_image' column\n",
    "            cropped_reshape_image = cv2.resize(cropped_image, (171, 138))\n",
    "\n",
    "            # Store the cropped image in the list\n",
    "            cropped_image_list.append(cropped_reshape_image)\n",
    "        else:\n",
    "            # If the coordinates are invalid, append a placeholder (e.g., None)\n",
    "            cropped_image_list.append(None)\n",
    "\n",
    "    # Add a new column to the DataFrame with cropped images\n",
    "    df['cropped_image'] = cropped_image_list.copy()\n",
    "\n",
    "    # Filter out rows with None values in the 'cropped_image' column\n",
    "    df_valid_crops = df.dropna(subset=['cropped_image']).copy()\n",
    "    \n",
    "    df_final = df_valid_crops[['class', 'cropped_image']].copy()\n",
    "    \n",
    "    # \n",
    "    old_class_to_actual_class = {\n",
    "        'tuna' : 'tuna', \n",
    "        'surgeon': 'surgeon', \n",
    "        'shark': 'shark', \n",
    "        'jack': 'jack', \n",
    "        'grouper': 'grouper', \n",
    "        'parrot': 'parrot', \n",
    "        'snapper': 'snapper',\n",
    "        'damsel': 'damsel', \n",
    "        'trigger': 'trigger', \n",
    "        'Zanclidae (Moorish Idol)': 'moorish idol',\n",
    "        'Scaridae -Parrotfishes-': 'parrot', \n",
    "        'Carangidae -Jacks-': 'jack',\n",
    "        'Scombridae -Tunas-': 'tuna', \n",
    "        'Shark -Selachimorpha-': 'shark',\n",
    "        'Serranidae -Groupers-': 'grouper', \n",
    "        'Lutjanidae -Snappers-': 'snapper',\n",
    "        'Acanthuridae -Surgeonfishes-': 'surgeon', \n",
    "        'Pomacentridae -Damselfishes-': 'damsel',\n",
    "        'Labridae -Wrasse-': 'wrasse', \n",
    "        'angel': 'angel', \n",
    "        'wrasse': 'wrasse', \n",
    "        'Zanclidae -Moorish Idol-': 'moorish idol',\n",
    "        'Ephippidae -Spadefishes-': 'spade', \n",
    "        'Pomacanthidae -Angelfishes-': 'angel',\n",
    "        'Balistidae -Triggerfishes-': 'trigger', \n",
    "        'spade': 'spade'\n",
    "    }\n",
    "   \n",
    "    # Replace old class names with new class names\n",
    "    df_final['class'] = df_final['class'].replace(old_class_to_actual_class)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def print_image_by_classes(df,images_to_plot=2):\n",
    "    \n",
    "    # Get unique classes in the DataFrame\n",
    "    unique_classes = df['class'].unique()\n",
    "\n",
    "    # Plot two random images for each class\n",
    "    for class_name in unique_classes:\n",
    "        # Filter DataFrame based on the current class\n",
    "        df_same_class = df[df['class'] == class_name]\n",
    "\n",
    "        # Check the number of images in the current class\n",
    "        num_images = len(df_same_class)\n",
    "\n",
    "        if num_images >= images_to_plot:\n",
    "            # Randomly select two images from the current class\n",
    "            random_indices = random.sample(df_same_class.index.tolist(), images_to_plot)\n",
    "\n",
    "            # Plot the two random images for the current class\n",
    "            fig, axes = plt.subplots(1, images_to_plot, figsize=(10, 5))\n",
    "            fig.suptitle(f'Two Random Images of Class: {class_name}')\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                ax.imshow(df_same_class.loc[random_indices[i]]['cropped_image'])\n",
    "                ax.set_title(f\"Index: {random_indices[i]}\")\n",
    "                ax.axis('off')\n",
    "\n",
    "            plt.show()\n",
    "            continue\n",
    "            \n",
    "        elif num_images >= 1:\n",
    "            # Plot the single image for the current class\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(df_same_class.iloc[0]['cropped_image'])\n",
    "            plt.title(f'Image of Class: {class_name} (Index: {df_same_class.index[0]})')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            print(f\"No images of class '{class_name}' for plotting.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to ressort the X and y of dataframe\n",
    "def to_work(df):\n",
    "    \n",
    "    class_to_number = {\n",
    "    'tuna': 0, \n",
    "    'surgeon': 1, \n",
    "    'shark': 2, \n",
    "    'jack': 3, \n",
    "    'grouper': 4, \n",
    "    'parrot': 5, \n",
    "    'snapper': 6,\n",
    "    'damsel': 7, \n",
    "    'trigger': 8, \n",
    "    'moorish idol': 9, \n",
    "    'wrasse': 10, \n",
    "    'angel': 11, \n",
    "    'spade': 12\n",
    "}\n",
    "\n",
    "    X = np.stack(df['cropped_image'].to_numpy().copy(), axis=0)\n",
    "    \n",
    "    # standardize and center data (make my pc crash)\n",
    "    X = (X / 255) - 0.5\n",
    "    \n",
    "    y = df['class'].replace(class_to_number).to_numpy().copy()\n",
    "    y_cat = to_categorical(y, num_classes=13)  \n",
    "    \n",
    "    return X, y, y_cat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_and_crop('data/train/')\n",
    "df_test = read_and_crop('data/test/')\n",
    "df_valid = read_and_crop('data/valid/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    \n",
    "    # Start by creating a sequential model\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4, 4), activation='relu', padding='same', input_shape=(138, 171, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    ### Second Convolution & MaxPoolingialize\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ### One Fully Connected layer\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    # droupout to minimise the overfitting\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    ### Last layer - Classification Layer\n",
    "    model.add(layers.Dense(13, activation='softmax')) # softmax for multiclass classification\n",
    "\n",
    "    ### Model compilation\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'Precision','Recall'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model's Trainning and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot the result\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(4,1, figsize=(10,10))\n",
    "    \n",
    "    axs[0].plot(history.history['loss'], color='red', label='train')\n",
    "    axs[0].plot(history.history['val_loss'], color='blue', label='val')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].legend(['train', 'validation'],loc=\"upper right\")\n",
    "    \n",
    "    axs[1].plot(history.history['accuracy'], color='red', label='train')\n",
    "    axs[1].plot(history.history['val_accuracy'], color='blue', label='val')\n",
    "    axs[1].set_title('Accuracy ')\n",
    "    axs[1].legend(['train', 'validation'],loc=\"upper right\")\n",
    "\n",
    "    axs[2].plot(history.history['precision'], color='red', label='train')\n",
    "    axs[2].plot(history.history['val_precision'], color='blue', label='val')\n",
    "    axs[2].set_title('Precision ')\n",
    "    axs[2].legend(['train', 'validation'],loc=\"upper right\")\n",
    "\n",
    "    axs[3].plot(history.history['recall'], color='red', label='train')\n",
    "    axs[3].plot(history.history['val_recall'], color='blue', label='val')\n",
    "    axs[3].set_title('Recall')\n",
    "    axs[3].legend(['train', 'validation'],loc=\"upper right\")\n",
    "    \n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='Epoch', ylabel='')\n",
    "\n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, y_train_cat = to_work(df_train)\n",
    "X_test, y_test, y_test_cat = to_work(df_test)\n",
    "X_val, y_val, y_val_cat = to_work(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning : Take around 20 min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the models & the metrics\n",
    "models_cnn = {}\n",
    "\n",
    "# early stopping critera\n",
    "es = EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "for batch in [4,16,32,64,128,256] :\n",
    "    \n",
    "    model = init_model()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs = 100,\n",
    "        batch_size = batch, \n",
    "        verbose = 0, \n",
    "        callbacks = [es]\n",
    "    )\n",
    "    \n",
    "    print(f'------------------------------------------Batch Size {batch}------------------------------------------')\n",
    "    \n",
    "    # store the model\n",
    "    models_cnn[batch] = model\n",
    "        \n",
    "    # plot the history of loss and accuracy\n",
    "    plot_history(history)\n",
    "    \n",
    "    # print the evaluation of the model:\n",
    "    trainEval = model.evaluate(X_train,y_train_cat, verbose=0)\n",
    "    valEval = model.evaluate(X_val,y_val_cat, verbose=0)\n",
    "\n",
    "    print(\"         Model Evaluation on Training :\")\n",
    "    print(\"     Training Loss:    \", trainEval[0])\n",
    "    print(\"   Training Accuracy:  \", trainEval[1])\n",
    "    print(\"  Training Precision:  \", trainEval[2])\n",
    "    print(\"    Training Recall:   \", trainEval[3], '\\n')\n",
    "    print(\"         Model Evaluation on Validation :\")\n",
    "    print(\"    Validation Loss:   \", valEval[0])\n",
    "    print(\"  Validation Accuracy: \", valEval[1])\n",
    "    print(\" Validation Precision: \", valEval[2])\n",
    "    print(\"   Validation Recall:  \", valEval[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for all of the test that the validation loss is going up as if the model is overfitting but most of time precision, recall and accuracy are going up or for batch size 128, 16 and 4 the precision is goind down but the recall is going up with the epochs. The overfitting may be caused by the underbalanced data in the training and validation. We can also see that the model with the best results is the model trained with a batch size of 16, it has the best loss and second best accuracy, precision and recall on validation by 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the 'best model' \n",
    "modelClassif = models_cnn[16]\n",
    "\n",
    "# print the evaluation of the model:\n",
    "testEval = modelClassif.evaluate(X_test,y_test_cat, verbose=0)\n",
    "\n",
    "print(\"         Model Evaluation on Test :\")\n",
    "print(\"    Test Loss:   \", testEval[0])\n",
    "print(\"  Test Accuracy: \", testEval[1])\n",
    "print(\" Test Precision: \", testEval[2])\n",
    "print(\"   Test Recall:  \", testEval[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_with_pretrained_base(base_model):\n",
    "    base_model.trainable = False  # Geler les couches du modèle pré-entraîné\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(13, activation='softmax')  # Adaptez à votre nombre de classes\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'Precision', 'Recall'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Chargement de VGG16 comme base\n",
    "base_model_vgg16 = VGG16(include_top=False, input_shape=(138, 171, 3), weights='imagenet')\n",
    "\n",
    "# Initialisation du modèle VGG16\n",
    "model_vgg16 = init_model_with_pretrained_base(base_model_vgg16)\n",
    "\n",
    "# Entraînement\n",
    "history_vgg16 = model_vgg16.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Évaluation\n",
    "plot_history(history_vgg16)\n",
    "\n",
    "# print the evaluation of the model:\n",
    "trainEval = model_vgg16.evaluate(X_train,y_train_cat, verbose=0)\n",
    "valEval = model_vgg16.evaluate(X_val,y_val_cat, verbose=0)\n",
    "valTest = model_vgg16.evaluate(X_test,y_test_cat, verbose=0)\n",
    "\n",
    "print(\"         Model Evaluation on Training :\")\n",
    "print(\"     Training Loss:    \", trainEval[0])\n",
    "print(\"   Training Accuracy:  \", trainEval[1])\n",
    "print(\"  Training Precision:  \", trainEval[2])\n",
    "print(\"    Training Recall:   \", trainEval[3], '\\n')\n",
    "print(\"         Model Evaluation on Validation :\")\n",
    "print(\"    Validation Loss:   \", valEval[0])\n",
    "print(\"  Validation Accuracy: \", valEval[1])\n",
    "print(\" Validation Precision: \", valEval[2])\n",
    "print(\"   Validation Recall:  \", valEval[3])\n",
    "print(\"         Model Evaluation on Test :\")\n",
    "print(\"    Validation Loss:   \", valTest[0])\n",
    "print(\"  Validation Accuracy: \", valTest[1])\n",
    "print(\" Validation Precision: \", valTest[2])\n",
    "print(\"   Validation Recall:  \", valTest[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantages\n",
    "\n",
    "- Haute précision et performance: atteignant une précision d'entraînement de 95.66% et une précision de validation de 66.97%.\n",
    "- Amélioration constante des métriques: Les métriques telles que la précision, la précision et le rappel ont toutes montré des améliorations constantes, indiquant que le modèle apprend efficacement à partir des données.\n",
    "\n",
    "Inconvénients\n",
    "\n",
    "- Temps d'entraînement plus long: Avec des temps d'étape d'environ 70 à 77 secondes, VGG16 peut être considéré comme ayant un temps d'entraînement relativement long par rapport aux autres modèles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 4s 0us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m model_resnet50 \u001b[38;5;241m=\u001b[39m init_model_with_pretrained_base(base_model_resnet50)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Entraînement\u001b[39;00m\n\u001b[1;32m     13\u001b[0m history_resnet50 \u001b[38;5;241m=\u001b[39m model_resnet50\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mX_train\u001b[49m, y_train_cat,\n\u001b[1;32m     15\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     16\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \n\u001b[1;32m     17\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     18\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     19\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Évaluation\u001b[39;00m\n\u001b[1;32m     23\u001b[0m plot_history(history_resnet50)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model_resnet50 = ResNet50(include_top=False, input_shape=(138, 171, 3), weights='imagenet')\n",
    "model_resnet50 = init_model_with_pretrained_base(base_model_resnet50)\n",
    "\n",
    "# Chargement de ResNet50 comme base\n",
    "base_model_resnet50 = ResNet50(include_top=False, input_shape=(138, 171, 3), weights='imagenet')\n",
    "\n",
    "# Initialisation du modèle avec ResNet50 comme base\n",
    "model_resnet50 = init_model_with_pretrained_base(base_model_resnet50)\n",
    "\n",
    "# Entraînement\n",
    "history_resnet50 = model_resnet50.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,  \n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Évaluation\n",
    "plot_history(history_resnet50)\n",
    "\n",
    "# print the evaluation of the model:\n",
    "trainEval = model_resnet50.evaluate(X_train,y_train_cat, verbose=0)\n",
    "valEval = model_resnet50.evaluate(X_val,y_val_cat, verbose=0)\n",
    "valTest = model_resnet50.evaluate(X_test,y_test_cat, verbose=0)\n",
    "\n",
    "print(\"         Model Evaluation on Training :\")\n",
    "print(\"     Training Loss:    \", trainEval[0])\n",
    "print(\"   Training Accuracy:  \", trainEval[1])\n",
    "print(\"  Training Precision:  \", trainEval[2])\n",
    "print(\"    Training Recall:   \", trainEval[3], '\\n')\n",
    "print(\"         Model Evaluation on Validation :\")\n",
    "print(\"    Validation Loss:   \", valEval[0])\n",
    "print(\"  Validation Accuracy: \", valEval[1])\n",
    "print(\" Validation Precision: \", valEval[2])\n",
    "print(\"   Validation Recall:  \", valEval[3])\n",
    "print(\"         Model Evaluation on Test :\")\n",
    "print(\"    Validation Loss:   \", valTest[0])\n",
    "print(\"  Validation Accuracy: \", valTest[1])\n",
    "print(\" Validation Precision: \", valTest[2])\n",
    "print(\"   Validation Recall:  \", valTest[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantages\n",
    "\n",
    "- Architecture résiduelle: L'utilisation de connexions résiduelles aide à lutter contre le problème de disparition du gradient, permettant d'entraîner des réseaux plus profonds.\n",
    "- Efficacité du temps d'entraînement: Avec des temps d'étape autour de 39 à 41 secondes, ResNet50 est plus de 2X plus rapide à l'entraînement que VGG16.\n",
    "\n",
    "Inconvénients\n",
    "\n",
    "- Faible performance: Le modèle a montré une faible performance sur les métriques d'évaluation, avec une précision d'entraînement de seulement 25.73% et une précision de validation encore plus faible.\n",
    "- Absence d'apprentissage: Le modèle n'a pas amélioré ses prédictions au-delà du hasard, comme en témoignent les précisions et rappels nuls sur les données de validation.\n",
    "\n",
    "=> Ne convient pas à nos données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "\n",
    "def init_model_with_efficientnet():\n",
    "    base_model = EfficientNetB5(include_top=False, weights='imagenet', input_shape=(138, 171, 3))\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    # Création du modèle\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(13, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'Precision', 'Recall'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du modèle avec EfficientNet\n",
    "model_efficientnet = init_model_with_efficientnet()\n",
    "\n",
    "# Entraînement\n",
    "history_efficientnet = model_efficientnet.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,  # Ajustez selon vos besoins\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Évaluation\n",
    "plot_history(history_efficientnet)\n",
    "\n",
    "# print the evaluation of the model:\n",
    "trainEval = model_efficientnet.evaluate(X_train,y_train_cat, verbose=0)\n",
    "valEval = model_efficientnet.evaluate(X_val,y_val_cat, verbose=0)\n",
    "valTest = model_efficientnet.evaluate(X_test,y_test_cat, verbose=0)\n",
    "\n",
    "print(\"         Model Evaluation on Training :\")\n",
    "print(\"     Training Loss:    \", trainEval[0])\n",
    "print(\"   Training Accuracy:  \", trainEval[1])\n",
    "print(\"  Training Precision:  \", trainEval[2])\n",
    "print(\"    Training Recall:   \", trainEval[3], '\\n')\n",
    "print(\"         Model Evaluation on Validation :\")\n",
    "print(\"    Validation Loss:   \", valEval[0])\n",
    "print(\"  Validation Accuracy: \", valEval[1])\n",
    "print(\" Validation Precision: \", valEval[2])\n",
    "print(\"   Validation Recall:  \", valEval[3])\n",
    "print(\"         Model Evaluation on Test :\")\n",
    "print(\"    Validation Loss:   \", valTest[0])\n",
    "print(\"  Validation Accuracy: \", valTest[1])\n",
    "print(\" Validation Precision: \", valTest[2])\n",
    "print(\"   Validation Recall:  \", valTest[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantages\n",
    "\n",
    "- Équilibrage des dimensions du modèle: EfficientNet utilise une approche systématique pour équilibrer la largeur, la profondeur et la résolution du réseau, ce qui peut conduire à une efficacité accrue.\n",
    "- Amélioration progressive: Bien que partant de performances initiales basses, le modèle montre une amélioration au fil des époques, suggérant une capacité d'apprentissage.\n",
    "\n",
    "Inconvénients\n",
    "\n",
    "- Performances basses: Le modèle commence avec des performances relativement basses, ce qui peut nécessiter plus de temps ou des ajustements pour atteindre une performance acceptable.\n",
    "- Complexité de l'architecture: L'équilibrage des facteurs de dimensionnement peut rendre l'architecture plus complexe à ajuster et à optimiser par rapport à des modèles plus simples.\n",
    "\n",
    "=> Le modèle ne convenait pas bien à notre jeu de donnée ou à cause de sa compléxité nous n'avons pas trouvé comment le régler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mobilnet_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "def init_model_with_mobilenetV2():\n",
    "    base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(138, 171, 3))\n",
    "    base_model.trainable = False  # Gel des couches du modèle pré-entraîné\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(13, activation='softmax')  # Adaptez en fonction du nombre de vos classes\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'Precision', 'Recall'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "2024/02/28 22:28:58 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'd7ea7dc1691d43759edcd2ff7ac6ce37', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52/52 [==============================] - 20s 303ms/step - loss: 1.1668 - accuracy: 0.6657 - precision: 0.7308 - recall: 0.5952 - val_loss: 0.5746 - val_accuracy: 0.8120 - val_precision: 0.8775 - val_recall: 0.7422\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 14s 265ms/step - loss: 0.3055 - accuracy: 0.9157 - precision: 0.9451 - recall: 0.8819 - val_loss: 0.3430 - val_accuracy: 0.8747 - val_precision: 0.9103 - val_recall: 0.8554\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 14s 267ms/step - loss: 0.1568 - accuracy: 0.9584 - precision: 0.9674 - recall: 0.9470 - val_loss: 0.3599 - val_accuracy: 0.8747 - val_precision: 0.8972 - val_recall: 0.8627\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 14s 270ms/step - loss: 0.0883 - accuracy: 0.9783 - precision: 0.9847 - recall: 0.9711 - val_loss: 0.2091 - val_accuracy: 0.9301 - val_precision: 0.9504 - val_recall: 0.9229\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 16s 315ms/step - loss: 0.0514 - accuracy: 0.9904 - precision: 0.9945 - recall: 0.9867 - val_loss: 0.2596 - val_accuracy: 0.9277 - val_precision: 0.9277 - val_recall: 0.9277\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 14s 270ms/step - loss: 0.0213 - accuracy: 0.9994 - precision: 0.9994 - recall: 0.9976 - val_loss: 0.1651 - val_accuracy: 0.9422 - val_precision: 0.9559 - val_recall: 0.9398\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 14s 266ms/step - loss: 0.0144 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1811 - val_accuracy: 0.9422 - val_precision: 0.9467 - val_recall: 0.9422\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - 14s 267ms/step - loss: 0.0098 - accuracy: 0.9994 - precision: 1.0000 - recall: 0.9988 - val_loss: 0.2125 - val_accuracy: 0.9373 - val_precision: 0.9416 - val_recall: 0.9325\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - 14s 270ms/step - loss: 0.0069 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9422 - val_precision: 0.9560 - val_recall: 0.9422\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - 14s 266ms/step - loss: 0.0049 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1662 - val_accuracy: 0.9422 - val_precision: 0.9560 - val_recall: 0.9422\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpcc64_572/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcc64_572/model/data/model/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model Evaluation on Training :\n",
      "     Training Loss:     0.035399071872234344\n",
      "   Training Accuracy:   0.9884337186813354\n",
      "  Training Precision:   0.9913001656532288\n",
      "    Training Recall:    0.9884337186813354 \n",
      "\n",
      "         Model Evaluation on Validation :\n",
      "    Validation Loss:    0.9832377433776855\n",
      "  Validation Accuracy:  0.7526881694793701\n",
      " Validation Precision:  0.7719298005104065\n",
      "   Validation Recall:   0.7434715628623962\n",
      "         Model Evaluation on Test :\n",
      "    Validation Loss:    0.884783148765564\n",
      "  Validation Accuracy:  0.7681159377098083\n",
      " Validation Precision:  0.7727272510528564\n",
      "   Validation Recall:   0.739130437374115\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du modèle avec MobileNetV2\n",
    "model_mobilenetV2 = init_model_with_mobilenetV2()\n",
    "\n",
    "# Entraînement\n",
    "history_mobilenetV2 = model_mobilenetV2.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,  # Ajustez selon vos besoins\n",
    "    batch_size=32,  # Ou tout autre nombre qui convient à votre configuration\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Évaluation\n",
    "plot_history(history_mobilenetV2)\n",
    "\n",
    "\n",
    "# print the evaluation of the model:\n",
    "trainEval = model_mobilenetV2.evaluate(X_train,y_train_cat, verbose=0)\n",
    "valEval = model_mobilenetV2.evaluate(X_val,y_val_cat, verbose=0)\n",
    "valTest = model_mobilenetV2.evaluate(X_test,y_test_cat, verbose=0)\n",
    "\n",
    "print(\"         Model Evaluation on Training :\")\n",
    "print(\"     Training Loss:    \", trainEval[0])\n",
    "print(\"   Training Accuracy:  \", trainEval[1])\n",
    "print(\"  Training Precision:  \", trainEval[2])\n",
    "print(\"    Training Recall:   \", trainEval[3], '\\n')\n",
    "print(\"         Model Evaluation on Validation :\")\n",
    "print(\"    Validation Loss:   \", valEval[0])\n",
    "print(\"  Validation Accuracy: \", valEval[1])\n",
    "print(\" Validation Precision: \", valEval[2])\n",
    "print(\"   Validation Recall:  \", valEval[3])\n",
    "print(\"         Model Evaluation on Test :\")\n",
    "print(\"    Validation Loss:   \", valTest[0])\n",
    "print(\"  Validation Accuracy: \", valTest[1])\n",
    "print(\" Validation Precision: \", valTest[2])\n",
    "print(\"   Validation Recall:  \", valTest[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantages:\n",
    "\n",
    "- Haute efficacité et performance: MobileNetV2 affiche des performances impressionnantes avec une précision, une précision et un rappel élevés sur les données d'entraînement, tout en conservant une bonne performance sur les données de validation. Cela indique une capacité de généralisation élevée malgré sa légèreté.\n",
    "- Rapidité d'entraînement: Le modèle est rapide à entraîner (a duré 2min en tout pour 10 époches)s.\n",
    "- Amélioration constante: Les scores de précision, de précision et de rappel s'améliorent régulièrement au fil des époques, montrant une bonne capacité d'apprentissage du modèle.\n",
    "\n",
    "Inconvénients:\n",
    "\n",
    "- Augmentation de la perte de validation: Bien que la précision de validation reste élevée, il y a une augmentation de la perte de validation dans les dernières époques, ce qui pourrait indiquer un début de surajustement.\n",
    "- Possibilité d'un sur-apprentissage: sur le dataset la précision ainsi que l'accuracy étaient très proche de 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison entre les modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(histories, names):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, history in zip(names, histories):\n",
    "        plt.plot(history.history['val_accuracy'], label=f'{name} val_accuracy')\n",
    "        plt.plot(history.history['accuracy'], '--', label=f'{name} train_accuracy')\n",
    "    plt.title('Accuracy Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, history in zip(names, histories):\n",
    "        plt.plot(history.history['val_loss'], label=f'{name} val_loss')\n",
    "        plt.plot(history.history['loss'], '--', label=f'{name} train_loss')\n",
    "    plt.title('Loss Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Supposons que vous ayez l'historique d'EfficientNet et de MobileNetV2, et potentiellement d'autres\n",
    "compare_models([history_vgg16, history_efficientnet, history_resnet50, history_mobilenetV2], ['VGG16','EfficientNet', 'ResNet50', 'MobileNetV2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 excelle en termes de performance brute, atteignant une haute précision et un rappel, mais au prix de temps d'entraînement plus longs et d'une plus grande consommation de ressources.\n",
    "ResNet50 et EfficientNet ont montré des performances initiales moins impressionnantes, potentiellement dues à des besoins de réglage fin des hyperparamètres ou à des caractéristiques spécifiques de l'ensemble de données.\n",
    "MobileNetV2 se distingue par son équilibre entre haute performance et efficacité, offrant une excellente option pour des applications nécessitant à la fois précision et rapidité, tout en étant conscient des ressources. Sa capacité à maintenir une haute précision avec moins de ressources le rend particulièrement attrayant pour des applications à réaliser rapidement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify 7 classes we need to change the amount of data. One overrepresented and 6 lacking a lot of data. Those are the target of our \"augmentation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_undersampling(X, y):\n",
    "    \n",
    "    # Reshape each image to a flat vector\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "        # summarize class distribution\n",
    "    print(f\"Original class distribution: {Counter(y)}\")\n",
    "    st = 376\n",
    "\n",
    "    oversample = RandomOverSampler(sampling_strategy={7:st,8:st,9:st,10:st,11:st,12:st})\n",
    "    undersample = RandomUnderSampler(sampling_strategy={1:st})\n",
    "\n",
    "        # Fit and apply the transform\n",
    "    X_over, y_over = oversample.fit_resample(X_flat, y)\n",
    "    X_under, y_under = undersample.fit_resample(X_over, y_over)\n",
    "        # Summarize class distribution after oversampling\n",
    "    print(f\"Class distribution after sampling: {Counter(y_under)}\")\n",
    "\n",
    "        # Reshape the resampled data back to the original image shape\n",
    "    X_resampled = X_under.reshape(-1, 138, 171, 3)\n",
    "\n",
    "    return X_resampled, y_under"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is tailored for the original fish training dataset. It applies the oversampling and the undersampling of the fish training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the \"augmented\" data\n",
    "X_resampled, y_resampled = over_undersampling(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have y as categorical after the augmentation\n",
    "y_resampled_cat=to_categorical(y_resampled, num_classes=13) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the models & the metrics\n",
    "models_cnn = {}\n",
    "\n",
    "# early stopping critera\n",
    "es = EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "for batch in [4,16,32,64,128,256] :\n",
    "    \n",
    "    model = init_model()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_resampled,\n",
    "        y_resampled_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs = 100,\n",
    "        batch_size = batch, \n",
    "        verbose = 0, \n",
    "        callbacks = [es]\n",
    "    )\n",
    "    \n",
    "    print(f'------------------------------------------Batch Size {batch}------------------------------------------')\n",
    "    \n",
    "    # store the model\n",
    "    models_cnn[batch] = model\n",
    "        \n",
    "    # plot the history of loss and accuracy\n",
    "    plot_history(history)\n",
    "    \n",
    "    # print the evaluation of the model:\n",
    "    trainEval = model.evaluate(X_resampled,y_resampled_cat, verbose=0)\n",
    "    valEval = model.evaluate(X_val,y_val_cat, verbose=0)\n",
    "\n",
    "    print(\"         Model Evaluation on Training :\")\n",
    "    print(\"     Training Loss:    \", trainEval[0])\n",
    "    print(\"   Training Accuracy:  \", trainEval[1])\n",
    "    print(\"  Training Precision:  \", trainEval[2])\n",
    "    print(\"    Training Recall:   \", trainEval[3], '\\n')\n",
    "    print(\"         Model Evaluation on Validation :\")\n",
    "    print(\"    Validation Loss:   \", valEval[0])\n",
    "    print(\"  Validation Accuracy: \", valEval[1])\n",
    "    print(\" Validation Precision: \", valEval[2])\n",
    "    print(\"   Validation Recall:  \", valEval[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de VGG16 comme base\n",
    "base_model_vgg16 = VGG16(include_top=False, input_shape=(138, 171, 3), weights='imagenet')\n",
    "\n",
    "# Initialisation du modèle VGG16\n",
    "model_vgg16 = init_model_with_pretrained_base(base_model_vgg16)\n",
    "\n",
    "# Entraînement\n",
    "history_vgg16 = model_vgg16.fit(\n",
    "    X_resampled, y_resampled_cat,\n",
    "    validation_split=(X_val,y_val_cat),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Évaluation\n",
    "plot_history(history_vgg16)\n",
    "\n",
    "# print the evaluation of the model:\n",
    "trainEval = model_vgg16.evaluate(X_resampled,y_resampled_cat, verbose=0)\n",
    "valEval = model_vgg16.evaluate(X_val,y_val_cat, verbose=0)\n",
    "valTest = model_vgg16.evaluate(X_test,y_test_cat, verbose=0)\n",
    "\n",
    "print(\"         Model Evaluation on Training :\")\n",
    "print(\"     Training Loss:    \", trainEval[0])\n",
    "print(\"   Training Accuracy:  \", trainEval[1])\n",
    "print(\"  Training Precision:  \", trainEval[2])\n",
    "print(\"    Training Recall:   \", trainEval[3], '\\n')\n",
    "print(\"         Model Evaluation on Validation :\")\n",
    "print(\"    Validation Loss:   \", valEval[0])\n",
    "print(\"  Validation Accuracy: \", valEval[1])\n",
    "print(\" Validation Precision: \", valEval[2])\n",
    "print(\"   Validation Recall:  \", valEval[3])\n",
    "print(\"         Model Evaluation on Test :\")\n",
    "print(\"    Validation Loss:   \", valTest[0])\n",
    "print(\"  Validation Accuracy: \", valTest[1])\n",
    "print(\" Validation Precision: \", valTest[2])\n",
    "print(\"   Validation Recall:  \", valTest[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mobilnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du modèle avec MobileNetV2\n",
    "model_mobilenetV2 = init_model_with_mobilenetV2()\n",
    "\n",
    "# Entraînement\n",
    "history_mobilenetV2 = model_mobilenetV2.fit(\n",
    "    X_resampled, y_resampled_cat,\n",
    "    validation_split=(X_val,y_val_cat),\n",
    "    epochs=10,  \n",
    "    batch_size=32,  \n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Évaluation\n",
    "plot_history(history_mobilenetV2)\n",
    "\n",
    "\n",
    "# print the evaluation of the model:\n",
    "trainEval = model_mobilenetV2.evaluate(X_train,y_train_cat, verbose=0)\n",
    "valEval = model_mobilenetV2.evaluate(X_val,y_val_cat, verbose=0)\n",
    "valTest = model_mobilenetV2.evaluate(X_test,y_test_cat, verbose=0)\n",
    "\n",
    "print(\"         Model Evaluation on Training :\")\n",
    "print(\"     Training Loss:    \", trainEval[0])\n",
    "print(\"   Training Accuracy:  \", trainEval[1])\n",
    "print(\"  Training Precision:  \", trainEval[2])\n",
    "print(\"    Training Recall:   \", trainEval[3], '\\n')\n",
    "print(\"         Model Evaluation on Validation :\")\n",
    "print(\"    Validation Loss:   \", valEval[0])\n",
    "print(\"  Validation Accuracy: \", valEval[1])\n",
    "print(\" Validation Precision: \", valEval[2])\n",
    "print(\"   Validation Recall:  \", valEval[3])\n",
    "print(\"         Model Evaluation on Test :\")\n",
    "print(\"    Validation Loss:   \", valTest[0])\n",
    "print(\"  Validation Accuracy: \", valTest[1])\n",
    "print(\" Validation Precision: \", valTest[2])\n",
    "print(\"   Validation Recall:  \", valTest[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todin/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(f'yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todin/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.19 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataYolo/data.yaml, epochs=1, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=26\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    756382  ultralytics.nn.modules.head.Detect           [26, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3015918 parameters, 3015902 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/todin/OneDrive/DeepLearning/Projet_DeepLearning/dataYolo/train/labels.cache... 944 images, 0 backgrounds, 0 corrupt: 100%|██████████| 944/944 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/todin/OneDrive/DeepLearning/Projet_DeepLearning/dataYolo/valid/labels.cache... 270 images, 0 backgrounds, 0 corrupt: 100%|██████████| 270/270 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/28 12:14:33 INFO mlflow.tracking.fluent: Experiment with name '/Shared/YOLOv8' does not exist. Creating a new experiment.\n",
      "2024/02/28 12:14:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n",
      "2024/02/28 12:14:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(8496a51abbf0438bbb79d36f35e7f9f1) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G       1.84      3.995      1.941        100        640: 100%|██████████| 59/59 [06:20<00:00,  6.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  11%|█         | 1/9 [00:09<01:13,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  22%|██▏       | 2/9 [00:18<01:06,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  33%|███▎      | 3/9 [00:28<00:56,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  44%|████▍     | 4/9 [00:36<00:44,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  56%|█████▌    | 5/9 [00:43<00:33,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  67%|██████▋   | 6/9 [00:50<00:23,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  78%|███████▊  | 7/9 [00:58<00:15,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  89%|████████▉ | 8/9 [01:05<00:07,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.700s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [01:09<00:00,  7.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651    0.00133     0.0383   0.000892    0.00031\n",
      "\n",
      "1 epochs completed in 0.128 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.19 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "Model summary (fused): 168 layers, 3010718 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  11%|█         | 1/9 [00:08<01:09,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  22%|██▏       | 2/9 [00:15<00:53,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  33%|███▎      | 3/9 [00:23<00:45,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  44%|████▍     | 4/9 [00:29<00:36,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  56%|█████▌    | 5/9 [00:36<00:28,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  67%|██████▋   | 6/9 [00:44<00:22,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  78%|███████▊  | 7/9 [00:51<00:14,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  89%|████████▉ | 8/9 [00:58<00:07,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.700s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [01:03<00:00,  7.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651     0.0022     0.0405    0.00126   0.000468\n",
      "Acanthuridae -Surgeonfishes-        270        104    0.00229     0.0288    0.00125   0.000288\n",
      "    Carangidae -Jacks-        270         46    0.00173       0.13    0.00106   0.000405\n",
      "     Labridae -Wrasse-        270          1          0          0          0          0\n",
      " Lutjanidae -Snappers-        270         62   0.000474     0.0161    0.00025   0.000175\n",
      "Scaridae -Parrotfishes-        270         21          0          0          0          0\n",
      "    Scombridae -Tunas-        270         23    0.00148       0.13   0.000995   0.000516\n",
      " Serranidae -Groupers-        270         30          0          0          0          0\n",
      " Shark -Selachimorpha-        270         33    0.00103     0.0909   0.000732   0.000262\n",
      "Zanclidae (Moorish Idol)        270          1          0          0          0          0\n",
      "Zanclidae -Moorish Idol-        270          1          0          0          0          0\n",
      "               grouper        270         30          0          0          0          0\n",
      "                  jack        270         46    0.00243       0.13    0.00151    0.00054\n",
      "                parrot        270         21          0          0          0          0\n",
      "                 shark        270         34    0.00145      0.118   0.000995   0.000307\n",
      "               snapper        270         62    0.00241     0.0323    0.00159   0.000737\n",
      "               surgeon        270        112     0.0164    0.00893    0.00919    0.00368\n",
      "                  tuna        270         23    0.00989     0.0435    0.00506    0.00152\n",
      "                wrasse        270          1          0          0          0          0\n",
      "Speed: 1.7ms preprocess, 94.5ms inference, 0.0ms loss, 130.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='dataYolo/data.yaml', epochs=1, batch=16, patience=10, optimizer='Adam', lr0=0.01, lrf=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.19 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataYolo/data.yaml, epochs=10, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=0, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    756382  ultralytics.nn.modules.head.Detect           [26, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3015918 parameters, 3015902 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/todin/OneDrive/DeepLearning/Projet_DeepLearning/dataYolo/train/labels.cache... 944 images, 0 backgrounds, 0 corrupt: 100%|██████████| 944/944 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/todin/OneDrive/DeepLearning/Projet_DeepLearning/dataYolo/valid/labels.cache... 270 images, 0 backgrounds, 0 corrupt: 100%|██████████| 270/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train4/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/28 15:50:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n",
      "2024/02/28 15:50:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(60210fa95ee3468887dc2aba2272d2ce) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mWARNING ⚠️ Failed to initialize: Changing param values is not allowed. Param with key='epochs' was already logged with value='100' for run ID='60210fa95ee3468887dc2aba2272d2ce'. Attempted logging new value '10'.\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mWARNING ⚠️ Not tracking this run\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.627      2.891      1.933         27        640: 100%|██████████| 59/59 [04:10<00:00,  4.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:30<00:00,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651     0.0819      0.162     0.0726     0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.608      2.818      1.904         68        640: 100%|██████████| 59/59 [04:30<00:00,  4.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:29<00:00,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.153      0.152     0.0525     0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.592      2.788      1.882         28        640: 100%|██████████| 59/59 [04:30<00:00,  4.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:25<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.107      0.184     0.0815     0.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.488      2.639      1.772         39        640: 100%|██████████| 59/59 [04:04<00:00,  4.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:29<00:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651     0.0889      0.203     0.0844     0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.528      2.651      1.806         33        640: 100%|██████████| 59/59 [04:07<00:00,  4.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:26<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651     0.0748      0.186     0.0716     0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.532      2.561      1.778         22        640: 100%|██████████| 59/59 [04:30<00:00,  4.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:37<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.144      0.289      0.123     0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.437      2.414      1.705         73        640: 100%|██████████| 59/59 [04:52<00:00,  4.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:29<00:00,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.166      0.282       0.14     0.0784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.396      2.353      1.648         48        640: 100%|██████████| 59/59 [05:29<00:00,  5.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:36<00:00,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.147      0.306      0.158     0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.348      2.284        1.6         52        640: 100%|██████████| 59/59 [06:12<00:00,  6.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:40<00:00,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651       0.14      0.308      0.138      0.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.266      2.154      1.555         47        640: 100%|██████████| 59/59 [06:13<00:00,  6.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:47<00:00,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.168      0.302      0.168      0.103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.906 hours.\n",
      "Optimizer stripped from runs/detect/train4/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from runs/detect/train4/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating runs/detect/train4/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.19 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "Model summary (fused): 168 layers, 3010718 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:34<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.169      0.301      0.167      0.102\n",
      "Acanthuridae -Surgeonfishes-        270        104      0.324      0.359      0.279      0.145\n",
      "    Carangidae -Jacks-        270         46      0.169      0.326      0.184      0.127\n",
      "     Labridae -Wrasse-        270          1          0          0          0          0\n",
      " Lutjanidae -Snappers-        270         62      0.208      0.323      0.174      0.108\n",
      "Scaridae -Parrotfishes-        270         21      0.147     0.0476     0.0847     0.0665\n",
      "    Scombridae -Tunas-        270         23      0.255      0.957      0.387      0.271\n",
      " Serranidae -Groupers-        270         30      0.281      0.233      0.219     0.0933\n",
      " Shark -Selachimorpha-        270         33      0.216      0.485      0.236      0.159\n",
      "Zanclidae (Moorish Idol)        270          1          0          0          0          0\n",
      "Zanclidae -Moorish Idol-        270          1          0          0          0          0\n",
      "               grouper        270         30      0.271      0.267      0.217     0.0885\n",
      "                  jack        270         46      0.184      0.304      0.177      0.118\n",
      "                parrot        270         21     0.0705     0.0476     0.0661     0.0507\n",
      "                 shark        270         34      0.264      0.265      0.235      0.161\n",
      "               snapper        270         62      0.174      0.339      0.147      0.088\n",
      "               surgeon        270        112      0.261      0.509      0.278      0.147\n",
      "                  tuna        270         23       0.22      0.957      0.317      0.221\n",
      "                wrasse        270          1          0          0          0          0\n",
      "Speed: 3.1ms preprocess, 114.7ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    }
   ],
   "source": [
    "result2 = model.train(data='dataYolo/data.yaml', epochs=10, batch=16, patience=10, optimizer='Adam', lr0=0.01, lrf=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with one epoch the model is not good, same with 10 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association of pre trained & Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "  fig = plt.figure(figsize=(20, 15))\n",
    "  plt.grid(False)\n",
    "  plt.imshow(image)\n",
    "\n",
    "  \n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color,\n",
    "                               font,\n",
    "                               thickness=4,\n",
    "                               display_str_list=()):\n",
    "  \"\"\"Adds a bounding box to an image.\"\"\"\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                ymin * im_height, ymax * im_height)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
    "             (left, top)],\n",
    "            width=thickness,\n",
    "            fill=color)\n",
    "\n",
    "  # If the total height of the display strings added to the top of the bounding\n",
    "  # box exceeds the top of the image, stack the strings below the bounding box\n",
    "  # instead of above.\n",
    "  display_str_heights = [font.getbbox(ds)[3] for ds in display_str_list]\n",
    "  # Each display_str has a top and bottom margin of 0.05x.\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = top + total_display_str_height\n",
    "  # Reverse list and print from bottom to top.\n",
    "  for display_str in display_str_list[::-1]:\n",
    "    bbox = font.getbbox(display_str)\n",
    "    text_width, text_height = bbox[2], bbox[3]\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
    "                    (left + text_width, text_bottom)],\n",
    "                   fill=color)\n",
    "    draw.text((left + margin, text_bottom - text_height - margin),\n",
    "              display_str,\n",
    "              fill=\"black\",\n",
    "              font=font)\n",
    "    text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "def draw_boxes(image, boxes, class_info):\n",
    "    \"\"\"Draw bounding boxes on the image.\"\"\"\n",
    "    colors = list(ImageColor.colormap.values())\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\", 25)\n",
    "    except IOError:\n",
    "        print(\"Font not found, using default font.\")\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    \n",
    "    for i in range (boxes.shape[0]):\n",
    "        ymin, xmin, ymax, xmax = boxes[i]\n",
    "        color = colors[hash(class_info[i]) % len(colors)]\n",
    "\n",
    "        # Draw bounding box on the image\n",
    "        draw_bounding_box_on_image(\n",
    "            image_pil,\n",
    "            ymin,\n",
    "            xmin,\n",
    "            ymax,\n",
    "            xmax,\n",
    "            color,\n",
    "            font,\n",
    "            display_str_list=[f'Class: {class_info[i]}'],\n",
    "        )\n",
    "        np.copyto(image, np.array(image_pil))\n",
    "\n",
    "    return image\n",
    "\n",
    "def load_img(path):\n",
    "  img = tf.io.read_file(path)\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  return img\n",
    "\n",
    "def run_detector(detector, path):\n",
    "  img = load_img(path)\n",
    "\n",
    "  converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "  start_time = time.time()\n",
    "  result = detector(converted_img)\n",
    "  end_time = time.time()\n",
    "\n",
    "  result = {key:value.numpy() for key,value in result.items()}\n",
    "\n",
    "  print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
    "  print(\"Inference time: \", end_time-start_time)\n",
    "\n",
    "  image_with_boxes = draw_boxes(\n",
    "      img.numpy(), result[\"detection_boxes\"],\n",
    "      result[\"detection_class_entities\"], result[\"detection_scores\"])\n",
    "\n",
    "  display_image(image_with_boxes)\n",
    "\n",
    "def detect_img_2(image_path):\n",
    "  start_time = time.time()\n",
    "  run_detector(detector, image_path)\n",
    "  end_time = time.time()\n",
    "  print(\"Inference time:\",end_time-start_time)\n",
    "  \n",
    "def resort_fish_boxes(boxes, class_names, scores, threshold=0.5):\n",
    "    fish_boxes = []\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if scores[i] >= threshold and class_names[i] == \"Fish\":\n",
    "            fish_boxes.append(boxes[i])\n",
    "\n",
    "    if len(fish_boxes) > 0:\n",
    "        fish_boxes = np.array(fish_boxes)\n",
    "        # Sort fish_boxes based on confidence scores\n",
    "        # sorted_indices = np.argsort(scores[fish_boxes[:, 0]])\n",
    "        # fish_boxes = fish_boxes[sorted_indices[::-1]]\n",
    "        return fish_boxes\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "2024-02-28 21:29:13.893423: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n"
     ]
    }
   ],
   "source": [
    "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\" #@param [\"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\", \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"]\n",
    "\n",
    "detector = hub.load(module_handle).signatures['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train/_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>xmax</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FishDataset232_png.rf.CC8AfGJBFvbo9JTlNPe4.jpg</td>\n",
       "      <td>0.187828</td>\n",
       "      <td>0.082099</td>\n",
       "      <td>0.85232</td>\n",
       "      <td>0.676879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FishDataset232_png.rf.CC8AfGJBFvbo9JTlNPe4.jpg</td>\n",
       "      <td>0.550713</td>\n",
       "      <td>0.627715</td>\n",
       "      <td>0.93842</td>\n",
       "      <td>0.959714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image      ymin      xmin  \\\n",
       "0  FishDataset232_png.rf.CC8AfGJBFvbo9JTlNPe4.jpg  0.187828  0.082099   \n",
       "1  FishDataset232_png.rf.CC8AfGJBFvbo9JTlNPe4.jpg  0.550713  0.627715   \n",
       "\n",
       "      ymax      xmax  class  \n",
       "0  0.85232  0.676879      1  \n",
       "1  0.93842  0.959714      1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = list()\n",
    "\n",
    "img = load_img('data/train/' + df_train['filename'][1])\n",
    "\n",
    "converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "start_time = time.time()\n",
    "result = detector(converted_img)\n",
    "end_time = time.time()\n",
    "\n",
    "fish_boxes = resort_fish_boxes(\n",
    "        result[\"detection_boxes\"],\n",
    "        result[\"detection_class_entities\"],\n",
    "        result[\"detection_scores\"],\n",
    "        threshold=0.5)\n",
    "\n",
    "if fish_boxes is  None:\n",
    "    print(\"No fish detected.\")\n",
    "\n",
    "for coord in fish_boxes:\n",
    "    # Extract coordinates\n",
    "    ymin, xmin, ymax, xmax = coord\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_img = img[int(ymin * img.shape[0]):int(ymax * img.shape[0]),\n",
    "                      int(xmin * img.shape[1]):int(xmax * img.shape[1])]\n",
    "    \n",
    "    img_np = np.array([cv2.resize(cropped_img.numpy(), (171, 138))])\n",
    "    \n",
    "    a = model_mobilenetV2.predict(img_np, verbose=0)\n",
    "    \n",
    "    array.append([df_train['filename'][1], ymin, xmin, ymax, xmax, np.argmax(a)])\n",
    "\n",
    "df_pred = pd.DataFrame(array, columns=['image','ymin', 'xmin', 'ymax', 'xmax', 'class'])\n",
    "\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image      ymin      xmin  \\\n",
      "0  data/train/FishDataset232_png.rf.CC8AfGJBFvbo9...  0.187828  0.082099   \n",
      "1  data/train/FishDataset232_png.rf.CC8AfGJBFvbo9...  0.550713  0.627715   \n",
      "\n",
      "      ymax      xmax  class  \n",
      "0  0.85232  0.676879      1  \n",
      "1  0.93842  0.959714      1  \n"
     ]
    }
   ],
   "source": [
    "def detect_and_classify_fish(image_path, detector, classification_model, threshold=0.5):\n",
    "    # Load the image\n",
    "    img = load_img(image_path)\n",
    "\n",
    "    # Run object detection\n",
    "    converted_img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "    result = detector(converted_img)\n",
    "\n",
    "    # Get fish boxes\n",
    "    fish_boxes = resort_fish_boxes(\n",
    "        result[\"detection_boxes\"],\n",
    "        result[\"detection_class_entities\"],\n",
    "        result[\"detection_scores\"],\n",
    "        threshold=threshold\n",
    "    )\n",
    "\n",
    "    if fish_boxes is None:\n",
    "        print(\"No fish detected.\")\n",
    "        return\n",
    "\n",
    "    # Initialize an array to store the results\n",
    "    array = []\n",
    "\n",
    "    for coord in fish_boxes:\n",
    "        # Extract coordinates\n",
    "        ymin, xmin, ymax, xmax = coord\n",
    "\n",
    "        # Crop the image\n",
    "        cropped_img = img[int(ymin * img.shape[0]):int(ymax * img.shape[0]),\n",
    "                          int(xmin * img.shape[1]):int(xmax * img.shape[1])]\n",
    "\n",
    "        # Resize and preprocess the cropped image for classification\n",
    "        img_np = np.array([cv2.resize(cropped_img.numpy(), (171, 138))])\n",
    "        prediction = classification_model.predict(img_np, verbose=0)\n",
    "\n",
    "        # Get the predicted class index\n",
    "        predicted_class = np.argmax(prediction)\n",
    "\n",
    "        # Append results to the array\n",
    "        array.append([image_path, ymin, xmin, ymax, xmax, predicted_class])\n",
    "        \n",
    "    image_with_boxes = draw_boxes(\n",
    "      img.numpy(), fish_boxes,\n",
    "      np.array(array)[:,-1])\n",
    "    \n",
    "    display_image(image_with_boxes)\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    df_pred = pd.DataFrame(array, columns=['image', 'ymin', 'xmin', 'ymax', 'xmax', 'class'])\n",
    "\n",
    "    return df_pred\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'data/train/' + df_train['filename'][1]\n",
    "df_results = detect_and_classify_fish(image_path, detector, model_mobilenetV2)\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
