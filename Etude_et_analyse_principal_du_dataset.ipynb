{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Deep Learning\n",
    "by Thomas ODIN, Ma√Øa JOUENNE et Benoit CATEZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import et telechargement des paquets necessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todin/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "2024-02-28 11:39:50.532853: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-28 11:39:50.575813: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-28 11:39:50.576804: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-28 11:39:51.575821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df_train = pd.read_csv('data/train/_annotations.csv')\n",
    "\n",
    "# Read and store cropped images\n",
    "cropped_image_list = []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    image_path = row['filename']\n",
    "    image = Image.open('data/train/' + image_path)\n",
    "\n",
    "    # Extract cropping coordinates\n",
    "    xmin, ymin, xmax, ymax = (\n",
    "        max(0, row['xmin']),\n",
    "        max(0, row['ymin']),\n",
    "        min(image.width, row['xmax']),\n",
    "        min(image.height, row['ymax'])\n",
    "    )\n",
    "\n",
    "    # Check if the adjusted coordinates are valid\n",
    "    if xmin < xmax and ymin < ymax:\n",
    "        # Crop the image\n",
    "        cropped_image = np.array(image.crop((xmin, ymin, xmax, ymax)))\n",
    "\n",
    "        # Store the cropped image in the list\n",
    "        cropped_image_list.append(cropped_image)\n",
    "    else:\n",
    "        # If the coordinates are invalid, append a placeholder (e.g., None)\n",
    "        cropped_image_list.append(None)\n",
    "\n",
    "# Add a new column to the DataFrame with cropped images\n",
    "df_train['cropped_image'] = cropped_image_list\n",
    "\n",
    "# Filter out rows with None values in the 'cropped_image' column\n",
    "df_valid_crops = df_train.dropna(subset=['cropped_image'])\n",
    "\n",
    "# Display general information\n",
    "print(\"Number of valid cropped images:\", len(df_valid_crops))\n",
    "print(\"Number of proposed images:\", len(df_train))\n",
    "print(\"Image sizes:\")\n",
    "print(df_valid_crops['cropped_image'].apply(lambda x: x.shape).value_counts())\n",
    "\n",
    "# Get unique classes in the DataFrame\n",
    "unique_classes = df_valid_crops['class'].unique()\n",
    "\n",
    "# Number of random images to plot for each class\n",
    "images_to_plot = 2\n",
    "\n",
    "# Plot two random images for each class\n",
    "for class_name in unique_classes:\n",
    "    # Filter DataFrame based on the current class\n",
    "    df_same_class = df_valid_crops[df_valid_crops['class'] == class_name]\n",
    "\n",
    "    # Check the number of images in the current class\n",
    "    num_images = len(df_same_class)\n",
    "\n",
    "    if num_images >= images_to_plot:\n",
    "        # Randomly select two images from the current class\n",
    "        random_indices = random.sample(df_same_class.index.tolist(), images_to_plot)\n",
    "\n",
    "        # Plot the two random images for the current class\n",
    "        fig, axes = plt.subplots(1, images_to_plot, figsize=(10, 5))\n",
    "        fig.suptitle(f'Two Random Images of Class: {class_name}')\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(df_same_class.loc[random_indices[i]]['cropped_image'])\n",
    "            ax.set_title(f\"Index: {random_indices[i]}\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    elif num_images == 1:\n",
    "        # Plot the single image for the current class\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(df_same_class.iloc[0]['cropped_image'])\n",
    "        plt.title(f'Image of Class: {class_name} (Index: {df_same_class.index[0]}), only one sample')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images of class '{class_name}' for plotting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the classes are the same but with different name and that some of the picture are very pixelized. So we are going to do some mapping for the classes and create a function to automate the recuperation of the data. We can also see that due to the cropping the image are all of different size so after we are going to resize them into the same size the biggest one. We can also see that some image are double but we won't do anythings about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_class_to_actual_class = {\n",
    "    'tuna' : 'tuna', \n",
    "    'surgeon': 'surgeon', \n",
    "    'shark': 'shark', \n",
    "    'jack': 'jack', \n",
    "    'grouper': 'grouper', \n",
    "    'parrot': 'parrot', \n",
    "    'snapper': 'snapper',\n",
    "    'damsel': 'damsel', \n",
    "    'trigger': 'trigger', \n",
    "    'Zanclidae (Moorish Idol)': 'moorish idol',\n",
    "    'Scaridae -Parrotfishes-': 'parrot', \n",
    "    'Carangidae -Jacks-': 'jack',\n",
    "    'Scombridae -Tunas-': 'tuna', \n",
    "    'Shark -Selachimorpha-': 'shark',\n",
    "    'Serranidae -Groupers-': 'grouper', \n",
    "    'Lutjanidae -Snappers-': 'snapper',\n",
    "    'Acanthuridae -Surgeonfishes-': 'surgeon', \n",
    "    'Pomacentridae -Damselfishes-': 'damsel',\n",
    "    'Labridae -Wrasse-': 'wrasse', \n",
    "    'angel': 'angel', \n",
    "    'wrasse': 'wrasse', \n",
    "    'Zanclidae -Moorish Idol-': 'moorish idol',\n",
    "    'Ephippidae -Spadefishes-': 'spade', \n",
    "    'Pomacanthidae -Angelfishes-': 'angel',\n",
    "    'Balistidae -Triggerfishes-': 'trigger', \n",
    "    'spade': 'spade'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy valid dataframe \n",
    "df_new_class = df_valid_crops.copy()\n",
    "\n",
    "# Replace old class names with new class names\n",
    "df_new_class['class'] = df_new_class['class'].replace(old_class_to_actual_class)\n",
    "\n",
    "# Get unique classes in the DataFrame\n",
    "unique_classes = df_new_class['class'].unique()\n",
    "\n",
    "# Number of random images to plot for each class\n",
    "images_to_plot = 2\n",
    "\n",
    "# Plot two random images for each class\n",
    "for class_name in unique_classes:\n",
    "    # Filter DataFrame based on the current class\n",
    "    df_same_class = df_new_class[df_new_class['class'] == class_name]\n",
    "\n",
    "    # Check the number of images in the current class\n",
    "    num_images = len(df_same_class)\n",
    "\n",
    "    if num_images >= images_to_plot:\n",
    "        # Randomly select two images from the current class\n",
    "        random_indices = random.sample(df_same_class.index.tolist(), images_to_plot)\n",
    "\n",
    "        # Plot the two random images for the current class\n",
    "        fig, axes = plt.subplots(1, images_to_plot, figsize=(10, 5))\n",
    "        fig.suptitle(f'Two Random Images of Class: {class_name}')\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(df_same_class.loc[random_indices[i]]['cropped_image'])\n",
    "            ax.set_title(f\"Index: {random_indices[i]}\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "    elif num_images == 1:\n",
    "        # Plot the single image for the current class\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(df_same_class.iloc[0]['cropped_image'])\n",
    "        plt.title(f'Image of Class: {class_name} (Index: {df_same_class.index[0]}), only one sample')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images of class '{class_name}' for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_class['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some of the data which is underreepresented as told in the website "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to look at the size of the images and reshape them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_crops['cropped_image'].apply(lambda x: x.shape).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the average image size\n",
    "index = df_valid_crops['cropped_image'].apply(lambda x: x.shape).value_counts().sort_index().index\n",
    "\n",
    "# Convert tuples to arrays\n",
    "index_as_arrays = np.array([np.array(x) for x in index])\n",
    "\n",
    "# Transpose the array to have dimensions in the order (height, width, channels)\n",
    "index_transposed = index_as_arrays.transpose()\n",
    "\n",
    "# Calculate the average along each dimension\n",
    "average_size = np.mean(index_transposed, axis=1)\n",
    "\n",
    "print(average_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target size for resizing\n",
    "target_size = (138, 171)  \n",
    "\n",
    "# Reshape the images in the 'cropped_image' column\n",
    "df_new_class['cropped_image'] = df_new_class['cropped_image'].apply(lambda x: cv2.resize(x, target_size) if x is not None else None)\n",
    "\n",
    "# Get unique classes in the DataFrame\n",
    "unique_classes = df_new_class['class'].unique()\n",
    "\n",
    "# Number of random images to plot for each class\n",
    "images_to_plot = 2\n",
    "\n",
    "# Plot two random images for each class\n",
    "for class_name in unique_classes:\n",
    "    # Filter DataFrame based on the current class\n",
    "    df_same_class = df_new_class[df_new_class['class'] == class_name]\n",
    "\n",
    "    # Check the number of images in the current class\n",
    "    num_images = len(df_same_class)\n",
    "\n",
    "    if num_images >= images_to_plot:\n",
    "        # Randomly select two images from the current class\n",
    "        random_indices = random.sample(df_same_class.index.tolist(), images_to_plot)\n",
    "\n",
    "        # Plot the two random images for the current class\n",
    "        fig, axes = plt.subplots(1, images_to_plot, figsize=(10, 5))\n",
    "        fig.suptitle(f'Two Random Images of Class: {class_name}')\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(df_same_class.loc[random_indices[i]]['cropped_image'])\n",
    "            ax.set_title(f\"Index: {random_indices[i]}\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "    elif num_images == 1:\n",
    "        # Plot the single image for the current class\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(df_same_class.iloc[0]['cropped_image'])\n",
    "        plt.title(f'Image of Class: {class_name} (Index: {df_same_class.index[0]}), only one sample')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images of class '{class_name}' for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for crop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_crop(path):\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    if path[-1] == '/':\n",
    "        df = pd.read_csv(path +'_annotations.csv')\n",
    "    else :\n",
    "        df = pd.read_csv(path +'/_annotations.csv')\n",
    "\n",
    "    # Read and store cropped images\n",
    "    cropped_image_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        image_path = row['filename']\n",
    "        if path[-1] == '/':\n",
    "            image = Image.open(path + image_path)\n",
    "        else :\n",
    "            image = Image.open(path + '/' + image_path)\n",
    "            \n",
    "\n",
    "        # Extract cropping coordinates\n",
    "        xmin, ymin, xmax, ymax = (\n",
    "            max(0, row['xmin']),\n",
    "            max(0, row['ymin']),\n",
    "            min(image.width, row['xmax']),\n",
    "            min(image.height, row['ymax'])\n",
    "        )\n",
    "\n",
    "        # Check if the adjusted coordinates are valid\n",
    "        if xmin < xmax and ymin < ymax:\n",
    "            \n",
    "            # Crop the image\n",
    "            cropped_image = np.array(image.crop((xmin, ymin, xmax, ymax))) \n",
    "\n",
    "            # Reshape the images in the 'cropped_image' column\n",
    "            cropped_reshape_image = cv2.resize(cropped_image, (171, 138))\n",
    "\n",
    "            # Store the cropped image in the list\n",
    "            cropped_image_list.append(cropped_reshape_image)\n",
    "        else:\n",
    "            # If the coordinates are invalid, append a placeholder (e.g., None)\n",
    "            cropped_image_list.append(None)\n",
    "\n",
    "    # Add a new column to the DataFrame with cropped images\n",
    "    df['cropped_image'] = cropped_image_list.copy()\n",
    "\n",
    "    # Filter out rows with None values in the 'cropped_image' column\n",
    "    df_valid_crops = df.dropna(subset=['cropped_image']).copy()\n",
    "    \n",
    "    df_final = df_valid_crops[['class', 'cropped_image']].copy()\n",
    "    \n",
    "    # \n",
    "    old_class_to_actual_class = {\n",
    "        'tuna' : 'tuna', \n",
    "        'surgeon': 'surgeon', \n",
    "        'shark': 'shark', \n",
    "        'jack': 'jack', \n",
    "        'grouper': 'grouper', \n",
    "        'parrot': 'parrot', \n",
    "        'snapper': 'snapper',\n",
    "        'damsel': 'damsel', \n",
    "        'trigger': 'trigger', \n",
    "        'Zanclidae (Moorish Idol)': 'moorish idol',\n",
    "        'Scaridae -Parrotfishes-': 'parrot', \n",
    "        'Carangidae -Jacks-': 'jack',\n",
    "        'Scombridae -Tunas-': 'tuna', \n",
    "        'Shark -Selachimorpha-': 'shark',\n",
    "        'Serranidae -Groupers-': 'grouper', \n",
    "        'Lutjanidae -Snappers-': 'snapper',\n",
    "        'Acanthuridae -Surgeonfishes-': 'surgeon', \n",
    "        'Pomacentridae -Damselfishes-': 'damsel',\n",
    "        'Labridae -Wrasse-': 'wrasse', \n",
    "        'angel': 'angel', \n",
    "        'wrasse': 'wrasse', \n",
    "        'Zanclidae -Moorish Idol-': 'moorish idol',\n",
    "        'Ephippidae -Spadefishes-': 'spade', \n",
    "        'Pomacanthidae -Angelfishes-': 'angel',\n",
    "        'Balistidae -Triggerfishes-': 'trigger', \n",
    "        'spade': 'spade'\n",
    "    }\n",
    "   \n",
    "    # Replace old class names with new class names\n",
    "    df_final['class'] = df_final['class'].replace(old_class_to_actual_class)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def print_image_by_classes(df,images_to_plot=2):\n",
    "    \n",
    "    # Get unique classes in the DataFrame\n",
    "    unique_classes = df['class'].unique()\n",
    "\n",
    "    # Plot two random images for each class\n",
    "    for class_name in unique_classes:\n",
    "        # Filter DataFrame based on the current class\n",
    "        df_same_class = df[df['class'] == class_name]\n",
    "\n",
    "        # Check the number of images in the current class\n",
    "        num_images = len(df_same_class)\n",
    "\n",
    "        if num_images >= images_to_plot:\n",
    "            # Randomly select two images from the current class\n",
    "            random_indices = random.sample(df_same_class.index.tolist(), images_to_plot)\n",
    "\n",
    "            # Plot the two random images for the current class\n",
    "            fig, axes = plt.subplots(1, images_to_plot, figsize=(10, 5))\n",
    "            fig.suptitle(f'Two Random Images of Class: {class_name}')\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                ax.imshow(df_same_class.loc[random_indices[i]]['cropped_image'])\n",
    "                ax.set_title(f\"Index: {random_indices[i]}\")\n",
    "                ax.axis('off')\n",
    "\n",
    "            plt.show()\n",
    "            continue\n",
    "            \n",
    "        elif num_images >= 1:\n",
    "            # Plot the single image for the current class\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(df_same_class.iloc[0]['cropped_image'])\n",
    "            plt.title(f'Image of Class: {class_name} (Index: {df_same_class.index[0]})')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            print(f\"No images of class '{class_name}' for plotting.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to ressort the X and y of dataframe\n",
    "def to_work(df):\n",
    "    \n",
    "    class_to_number = {\n",
    "    'tuna': 0, \n",
    "    'surgeon': 1, \n",
    "    'shark': 2, \n",
    "    'jack': 3, \n",
    "    'grouper': 4, \n",
    "    'parrot': 5, \n",
    "    'snapper': 6,\n",
    "    'damsel': 7, \n",
    "    'trigger': 8, \n",
    "    'moorish idol': 9, \n",
    "    'wrasse': 10, \n",
    "    'angel': 11, \n",
    "    'spade': 12\n",
    "}\n",
    "\n",
    "    X = np.stack(df['cropped_image'].to_numpy().copy(), axis=0)\n",
    "    \n",
    "    # standardize and center data (make my pc crash)\n",
    "    X = (X / 255) - 0.5\n",
    "    \n",
    "    y = df['class'].replace(class_to_number).to_numpy().copy()\n",
    "    y_cat = to_categorical(y, num_classes=13)  \n",
    "    \n",
    "    return X, y, y_cat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_and_crop('data/train/')\n",
    "df_test = read_and_crop('data/test/')\n",
    "df_valid = read_and_crop('data/valid/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    \n",
    "    # Start by creating a sequential model\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4, 4), activation='relu', padding='same', input_shape=(138, 171, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    ### Second Convolution & MaxPoolingialize\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ### One Fully Connected layer\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    # droupout to minimise the overfitting\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    ### Last layer - Classification Layer\n",
    "    model.add(layers.Dense(13, activation='softmax')) # softmax for multiclass classification\n",
    "\n",
    "    ### Model compilation\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'Precision','Recall'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model's Trainning and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot the result\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(4,1, figsize=(10,10))\n",
    "    \n",
    "    axs[0].plot(history.history['loss'], color='red', label='train')\n",
    "    axs[0].plot(history.history['val_loss'], color='blue', label='val')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].legend(['train', 'validation'],loc=\"upper right\")\n",
    "    \n",
    "    axs[1].plot(history.history['accuracy'], color='red', label='train')\n",
    "    axs[1].plot(history.history['val_accuracy'], color='blue', label='val')\n",
    "    axs[1].set_title('Accuracy ')\n",
    "    axs[1].legend(['train', 'validation'],loc=\"upper right\")\n",
    "\n",
    "    axs[2].plot(history.history['precision'], color='red', label='train')\n",
    "    axs[2].plot(history.history['val_precision'], color='blue', label='val')\n",
    "    axs[2].set_title('Precision ')\n",
    "    axs[2].legend(['train', 'validation'],loc=\"upper right\")\n",
    "\n",
    "    axs[3].plot(history.history['recall'], color='red', label='train')\n",
    "    axs[3].plot(history.history['val_recall'], color='blue', label='val')\n",
    "    axs[3].set_title('Recall')\n",
    "    axs[3].legend(['train', 'validation'],loc=\"upper right\")\n",
    "    \n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='Epoch', ylabel='')\n",
    "\n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, y_train_cat = to_work(df_train)\n",
    "X_test, y_test, y_test_cat = to_work(df_test)\n",
    "X_val, y_val, y_val_cat = to_work(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning : Take around 20 min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the models & the metrics\n",
    "models_cnn = {}\n",
    "\n",
    "# early stopping critera\n",
    "es = EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "for batch in [4,16,32,64,128,256] :\n",
    "    \n",
    "    model = init_model()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs = 100,\n",
    "        batch_size = batch, \n",
    "        verbose = 0, \n",
    "        callbacks = [es]\n",
    "    )\n",
    "    \n",
    "    print(f'------------------------------------------Batch Size {batch}------------------------------------------')\n",
    "    \n",
    "    # store the model\n",
    "    models_cnn[batch] = model\n",
    "        \n",
    "    # plot the history of loss and accuracy\n",
    "    plot_history(history)\n",
    "    \n",
    "    # print the evaluation of the model:\n",
    "    trainEval = model.evaluate(X_train,y_train_cat, verbose=0)\n",
    "    valEval = model.evaluate(X_val,y_val_cat, verbose=0)\n",
    "\n",
    "    print(\"         Model Evaluation on Training :\")\n",
    "    print(\"     Training Loss:    \", trainEval[0])\n",
    "    print(\"   Training Accuracy:  \", trainEval[1])\n",
    "    print(\"  Training Precision:  \", trainEval[2])\n",
    "    print(\"    Training Recall:   \", trainEval[3], '\\n')\n",
    "    print(\"         Model Evaluation on Validation :\")\n",
    "    print(\"    Validation Loss:   \", valEval[0])\n",
    "    print(\"  Validation Accuracy: \", valEval[1])\n",
    "    print(\" Validation Precision: \", valEval[2])\n",
    "    print(\"   Validation Recall:  \", valEval[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for all of the test that the validation loss is going up as if the model is overfitting but most of time precision, recall and accuracy are going up or for batch size 128, 16 and 4 the precision is goind down but the recall is going up with the epochs. The overfitting may be caused by the underbalanced data in the training and validation. We can also see that the model with the best results is the model trained with a batch size of 16, it has the best loss and second best accuracy, precision and recall on validation by 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the 'best model' \n",
    "modelClassif = models_cnn[16]\n",
    "\n",
    "# print the evaluation of the model:\n",
    "testEval = modelClassif.evaluate(X_test,y_test_cat, verbose=0)\n",
    "\n",
    "print(\"         Model Evaluation on Test :\")\n",
    "print(\"    Test Loss:   \", testEval[0])\n",
    "print(\"  Test Accuracy: \", testEval[1])\n",
    "print(\" Test Precision: \", testEval[2])\n",
    "print(\"   Test Recall:  \", testEval[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to read the image and change the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    if path[-1] == '/':\n",
    "        df = pd.read_csv(path +'_annotations.csv')\n",
    "    else :\n",
    "        df = pd.read_csv(path +'/_annotations.csv')\n",
    "\n",
    "    # Read and store cropped images\n",
    "    image_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        image_path = row['filename']\n",
    "        if path[-1] == '/':\n",
    "            image = Image.open(path + image_path)\n",
    "        else :\n",
    "            image = Image.open(path + '/' + image_path)\n",
    "            \n",
    "        image = np.array(image) \n",
    "    \n",
    "        image_list.append(image)\n",
    "\n",
    "    # Add a new column to the DataFrame with cropped images\n",
    "    df['image'] = image_list.copy()\n",
    "\n",
    "    # Filter out rows with None values in the 'cropped_image' column\n",
    "    df_valid_crops = df.dropna(subset=['image']).copy()\n",
    "    \n",
    "    df_final = df_valid_crops[['class', 'image', 'ymin', 'xmin', 'xmax', 'ymax']].copy()\n",
    "    \n",
    "    # \n",
    "    old_class_to_actual_class = {\n",
    "        'tuna' : 'tuna', \n",
    "        'surgeon': 'surgeon', \n",
    "        'shark': 'shark', \n",
    "        'jack': 'jack', \n",
    "        'grouper': 'grouper', \n",
    "        'parrot': 'parrot', \n",
    "        'snapper': 'snapper',\n",
    "        'damsel': 'damsel', \n",
    "        'trigger': 'trigger', \n",
    "        'Zanclidae (Moorish Idol)': 'moorish idol',\n",
    "        'Scaridae -Parrotfishes-': 'parrot', \n",
    "        'Carangidae -Jacks-': 'jack',\n",
    "        'Scombridae -Tunas-': 'tuna', \n",
    "        'Shark -Selachimorpha-': 'shark',\n",
    "        'Serranidae -Groupers-': 'grouper', \n",
    "        'Lutjanidae -Snappers-': 'snapper',\n",
    "        'Acanthuridae -Surgeonfishes-': 'surgeon', \n",
    "        'Pomacentridae -Damselfishes-': 'damsel',\n",
    "        'Labridae -Wrasse-': 'wrasse', \n",
    "        'angel': 'angel', \n",
    "        'wrasse': 'wrasse', \n",
    "        'Zanclidae -Moorish Idol-': 'moorish idol',\n",
    "        'Ephippidae -Spadefishes-': 'spade', \n",
    "        'Pomacanthidae -Angelfishes-': 'angel',\n",
    "        'Balistidae -Triggerfishes-': 'trigger', \n",
    "        'spade': 'spade'\n",
    "    }\n",
    "   \n",
    "    # Replace old class names with new class names\n",
    "    df_final['class'] = df_final['class'].replace(old_class_to_actual_class)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def to_work2(df):\n",
    "    \n",
    "    class_to_number = {\n",
    "    'tuna': 0, \n",
    "    'surgeon': 1, \n",
    "    'shark': 2, \n",
    "    'jack': 3, \n",
    "    'grouper': 4, \n",
    "    'parrot': 5, \n",
    "    'snapper': 6,\n",
    "    'damsel': 7, \n",
    "    'trigger': 8, \n",
    "    'moorish idol': 9, \n",
    "    'wrasse': 10, \n",
    "    'angel': 11, \n",
    "    'spade': 12\n",
    "    }\n",
    "\n",
    "    X = np.stack(df['image'].to_numpy().copy(), axis=0)\n",
    "    \n",
    "    # standardize and center data (make my pc crash)\n",
    "    X = (X / 255) - 0.5\n",
    "    \n",
    "    y_class = df['class'].replace(class_to_number).to_numpy().copy()\n",
    "    \n",
    "    y_cat = to_categorical(y_class, num_classes=13)     \n",
    "    \n",
    "    y_coor = df[['ymin', 'ymax', 'xmin', 'xmax']].to_numpy().copy  \n",
    "    \n",
    "    return X, y_class, y_cat, y_coor \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and split the data to prepare for trainning\n",
    "detectionTrain = read('data/train')\n",
    "detectionValid = read('data/valid')\n",
    "detectionTest = read('data/test')\n",
    "\n",
    "X_dTrain, y_dTrain_class, y_dTrain_cat, y_dTrain_coor = to_work2(detectionTrain)\n",
    "X_dValid, y_dValid_class,y_dValid_cat, y_dValid_coor = to_work2(detectionValid)\n",
    "X_dTest, y_dTest_class,y_dTest_cat, y_dTest_coor = to_work2(detectionTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todin/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(f'yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todin/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.19 üöÄ Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataYolo/data.yaml, epochs=1, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=26\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    756382  ultralytics.nn.modules.head.Detect           [26, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3015918 parameters, 3015902 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/todin/OneDrive/DeepLearning/Projet_DeepLearning/dataYolo/train/labels.cache... 944 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 944/944 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/todin/OneDrive/DeepLearning/Projet_DeepLearning/dataYolo/valid/labels.cache... 270 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 270/270 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/28 12:14:33 INFO mlflow.tracking.fluent: Experiment with name '/Shared/YOLOv8' does not exist. Creating a new experiment.\n",
      "2024/02/28 12:14:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n",
      "2024/02/28 12:14:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(8496a51abbf0438bbb79d36f35e7f9f1) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G       1.84      3.995      1.941        100        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [06:20<00:00,  6.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  11%|‚ñà         | 1/9 [00:09<01:13,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  22%|‚ñà‚ñà‚ñè       | 2/9 [00:18<01:06,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  33%|‚ñà‚ñà‚ñà‚ñé      | 3/9 [00:28<00:56,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4/9 [00:36<00:44,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/9 [00:43<00:33,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6/9 [00:50<00:23,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7/9 [00:58<00:15,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [01:05<00:07,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 2.700s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [01:09<00:00,  7.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651    0.00133     0.0383   0.000892    0.00031\n",
      "\n",
      "1 epochs completed in 0.128 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.19 üöÄ Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "Model summary (fused): 168 layers, 3010718 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  11%|‚ñà         | 1/9 [00:08<01:09,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  22%|‚ñà‚ñà‚ñè       | 2/9 [00:15<00:53,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  33%|‚ñà‚ñà‚ñà‚ñé      | 3/9 [00:23<00:45,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4/9 [00:29<00:36,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/9 [00:36<00:28,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6/9 [00:44<00:22,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7/9 [00:51<00:14,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [00:58<00:07,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 2.700s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [01:03<00:00,  7.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651     0.0022     0.0405    0.00126   0.000468\n",
      "Acanthuridae -Surgeonfishes-        270        104    0.00229     0.0288    0.00125   0.000288\n",
      "    Carangidae -Jacks-        270         46    0.00173       0.13    0.00106   0.000405\n",
      "     Labridae -Wrasse-        270          1          0          0          0          0\n",
      " Lutjanidae -Snappers-        270         62   0.000474     0.0161    0.00025   0.000175\n",
      "Scaridae -Parrotfishes-        270         21          0          0          0          0\n",
      "    Scombridae -Tunas-        270         23    0.00148       0.13   0.000995   0.000516\n",
      " Serranidae -Groupers-        270         30          0          0          0          0\n",
      " Shark -Selachimorpha-        270         33    0.00103     0.0909   0.000732   0.000262\n",
      "Zanclidae (Moorish Idol)        270          1          0          0          0          0\n",
      "Zanclidae -Moorish Idol-        270          1          0          0          0          0\n",
      "               grouper        270         30          0          0          0          0\n",
      "                  jack        270         46    0.00243       0.13    0.00151    0.00054\n",
      "                parrot        270         21          0          0          0          0\n",
      "                 shark        270         34    0.00145      0.118   0.000995   0.000307\n",
      "               snapper        270         62    0.00241     0.0323    0.00159   0.000737\n",
      "               surgeon        270        112     0.0164    0.00893    0.00919    0.00368\n",
      "                  tuna        270         23    0.00989     0.0435    0.00506    0.00152\n",
      "                wrasse        270          1          0          0          0          0\n",
      "Speed: 1.7ms preprocess, 94.5ms inference, 0.0ms loss, 130.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='dataYolo/data.yaml', epochs=1, batch=16, patience=10, optimizer='Adam', lr0=0.01, lrf=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.19 üöÄ Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataYolo/data.yaml, epochs=10, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=0, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    756382  ultralytics.nn.modules.head.Detect           [26, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3015918 parameters, 3015902 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/todin/OneDrive/DeepLearning/Projet_DeepLearning/dataYolo/train/labels.cache... 944 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 944/944 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/todin/OneDrive/DeepLearning/Projet_DeepLearning/dataYolo/valid/labels.cache... 270 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 270/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train4/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/28 15:50:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n",
      "2024/02/28 15:50:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(60210fa95ee3468887dc2aba2272d2ce) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mWARNING ‚ö†Ô∏è Failed to initialize: Changing param values is not allowed. Param with key='epochs' was already logged with value='100' for run ID='60210fa95ee3468887dc2aba2272d2ce'. Attempted logging new value '10'.\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mWARNING ‚ö†Ô∏è Not tracking this run\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.627      2.891      1.933         27        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [04:10<00:00,  4.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:30<00:00,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651     0.0819      0.162     0.0726     0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.608      2.818      1.904         68        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [04:30<00:00,  4.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:29<00:00,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.153      0.152     0.0525     0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.592      2.788      1.882         28        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [04:30<00:00,  4.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:25<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.107      0.184     0.0815     0.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.488      2.639      1.772         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [04:04<00:00,  4.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:29<00:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651     0.0889      0.203     0.0844     0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.528      2.651      1.806         33        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [04:07<00:00,  4.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:26<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651     0.0748      0.186     0.0716     0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.532      2.561      1.778         22        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [04:30<00:00,  4.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:37<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.144      0.289      0.123     0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.437      2.414      1.705         73        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [04:52<00:00,  4.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:29<00:00,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.166      0.282       0.14     0.0784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.396      2.353      1.648         48        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [05:29<00:00,  5.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:36<00:00,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.147      0.306      0.158     0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.348      2.284        1.6         52        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [06:12<00:00,  6.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:40<00:00,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651       0.14      0.308      0.138      0.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.266      2.154      1.555         47        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [06:13<00:00,  6.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:47<00:00,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.168      0.302      0.168      0.103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.906 hours.\n",
      "Optimizer stripped from runs/detect/train4/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from runs/detect/train4/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating runs/detect/train4/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.19 üöÄ Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "Model summary (fused): 168 layers, 3010718 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:34<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        270        651      0.169      0.301      0.167      0.102\n",
      "Acanthuridae -Surgeonfishes-        270        104      0.324      0.359      0.279      0.145\n",
      "    Carangidae -Jacks-        270         46      0.169      0.326      0.184      0.127\n",
      "     Labridae -Wrasse-        270          1          0          0          0          0\n",
      " Lutjanidae -Snappers-        270         62      0.208      0.323      0.174      0.108\n",
      "Scaridae -Parrotfishes-        270         21      0.147     0.0476     0.0847     0.0665\n",
      "    Scombridae -Tunas-        270         23      0.255      0.957      0.387      0.271\n",
      " Serranidae -Groupers-        270         30      0.281      0.233      0.219     0.0933\n",
      " Shark -Selachimorpha-        270         33      0.216      0.485      0.236      0.159\n",
      "Zanclidae (Moorish Idol)        270          1          0          0          0          0\n",
      "Zanclidae -Moorish Idol-        270          1          0          0          0          0\n",
      "               grouper        270         30      0.271      0.267      0.217     0.0885\n",
      "                  jack        270         46      0.184      0.304      0.177      0.118\n",
      "                parrot        270         21     0.0705     0.0476     0.0661     0.0507\n",
      "                 shark        270         34      0.264      0.265      0.235      0.161\n",
      "               snapper        270         62      0.174      0.339      0.147      0.088\n",
      "               surgeon        270        112      0.261      0.509      0.278      0.147\n",
      "                  tuna        270         23       0.22      0.957      0.317      0.221\n",
      "                wrasse        270          1          0          0          0          0\n",
      "Speed: 3.1ms preprocess, 114.7ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    }
   ],
   "source": [
    "result2 = model.train(data='dataYolo/data.yaml', epochs=10, batch=16, patience=10, optimizer='Adam', lr0=0.01, lrf=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
